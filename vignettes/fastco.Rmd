---
title: "fastco: Fast Covariance, Correlation, and Cosine Similarity"
author: "Drew Schmidt"
date: "`r Sys.Date()`"
output:
  html_document:
    fig_caption: true
    toc: true
    number_sections: true
    css: include/custom.css
    highlight: kate
bibliography: include/fastco.bib
csl: "include/ieee.csl"
vignette: >
  %\VignetteIndexEntry{fastco: Fast Correlation, Covariance, and Cosine Similarity}
  %\VignetteEngine{knitr::rmarkdown}
---

```{r, echo=FALSE}
library(memuse)
matmemsize <- function(n) capture.output(memuse::mu(8*2*(n+1)*n))
```

# Introduction

**fastco** [[@fastco]] is a micro-package for computing cosine similarity in a quick, memory efficient way.  If you can do this better, I'd love to know how.

The package has separate routines for dense matrices/vectors and for a sparse matrix (like a term-document/document-term matrix). The use of each is seamless to the user by way of R's S3 methods.

Both the dense and sparse routines are written in C and should be very efficient.  The dense implementations here have been ruthlessly optimized; there's not much you can do for sparse matrices, but they should be fast enough if your data is very sparse.



## Why Put this in a Package?

There is no function built into R to compute the all-pairwise cosine similarities of the columns of a matrix.  As such, you will find people across the R community in a myriad of bizarre, often inefficient, ways.  In fact, some of them are even incorrect!

The operation `cosine()` provided by this package is a simple one, but it is important enough, particularly to fields like text mining, to have a good, high performance implementation.



## Installation

To install the R package:

```r
devtools::install_github("wrathematics/fastco")
```

The source code is also separated from the necessary R wrapper code.  So it easily builds as a shared library after removing `src/wrapper.c`.



## BLAS Library

When building/using this package, you will see the biggest performance improvements, in decreasing order of value, by using:

1. A good BLAS library
2. A compiler supporting OpenMP (preferably version 4 or better).

R ships the reference BLAS [[@lawson1979basic]] which are known to have poor performance.  Modern re-implementations of the BLAS library have identical API and should generally produce similar outputs from the same input, but they are instrumented to be very high performance.

Several well-supported high performance BLAS libraries used today are Intel MKL [[@mkl]] and AMD ACML [[@acml]], both of which are proprietary and can, depending on circumstances, require paying for a license to use.  There are also good open source implementations, such as OpenBLAS [[@openblas]], which is perhaps the best of the free options.  Another free BLAS library which will outperform the reference BLAS is Atlas [[@atlas]], although OpenBLAS will outperform it, so if possible, one should choose OpenBLAS over Atlas. In addition to merely being faster on a single core, all of the above named BLAS libraries except for Atlas is multi-threaded.

If you're on Linux, you can very easily use OpenBLAS with R.  For example, on Ubuntu you can simply run:

```
sudo apt-get install libopenblas-dev
sudo update-alternatives --config libblas.so.3
```

Users on other platforms like Windows or Mac (which I know considerably less about) might consider using Revolution R Open, which ships with Intel MKL.





# Cosine Similarity

## Definition

Given two vectors $x$ and $y$ each of length $m$, we can define the *cosine similarity* of the two vectors as 

$$
cosim(x,y) = \frac{x\cdot y}{\|x\| \|y\|}
$$

This is the cosine of the angle between the two vectors.  This is very similar to pearson correlation.  In fact, if the vectors $x$ and $y$ have their means removed, it is identical.  

$$
\rho(x,y) = \frac{(x - \bar{x}) \cdot (y - \bar{y})}{\|x - \bar{x}\| \|y - \bar{y}\|}
$$

Given an $m\times n$ matrix $A$, we can define the *cosine similarity* by extending the above definition to

$$
cosim(A) = \left[ cosim(A_{*,i}, A_{*,j}) \right]_{n\times n}
$$

Where $A_{*,i}$ denotes the i'th column vector of $A$.  In words, this is the matrix of all possible pairwise cosine similarities of the columns of the matrix $A$.



## Computing Cosines


### A Naive Implementation

Proceeding directly from the definition, we can easily compute the cosine of two vectors as follows:

```{r}
cosine <- function(x, y)
{
  cp <- t(x) %*% y
  normx <- sqrt(t(x) %*% x)
  normy <- sqrt(t(y) %*% y)
  cp / normx / normy
}
```

This might lead us to generalize to a matrix by defining the function to operate recursively or iteratively on the above construction.  In fact, this is precisely how the **lsa** package [[@lsa]] computes cosine similarity.  And while this captures the mathematical spirit of the operation, it ignores how computers actually operate on data in some critical ways.

Notably, this will only use the so-called "Level 1" BLAS.  BLAS operations are enumerated 1 through 3, for vector-vector, matrix-vector, and matrix-matrix operations.  The higher level operations are much more cache efficient, and so can achieve significant performance improvements over the lower level BLAS operations.  Since we wish to perform a full crossproduct (what numerical people call a "rank-k update"), we will achieve much better performance with the level 2 BLAS operations.


### A runtime efficient implementation in R

We can massively improve the performance by being slightly more careful in which R functions we use.  Consider for exaple:

```{r}
cosine <- function(x)
{
  cp <- crossprod(x)
  rtdg <- sqrt(diag(cp))
  cos <- cp / tcrossprod(rtdg)
  return(cos)
}
```

The main reason this will *significantly* outperform naive implementations is because it makes very efficient use of the BLAS; this of course assumes you are using efficient BLAS implementations.  Additionally, `crossprod()` and `tcrossprod()` are actually optimized to not only avoid the unnecessary copy in explicitly forming the transpose (produced when `t()` is called), but only compute one triangle and copy it over to the other, essentially doing only half the work.  These operations alone allow for significant improvement in performance:

```{r, cache=TRUE}
library(microbenchmark)
n <- 500
x <- matrix(rnorm(n*n), n, n)

mb <- microbenchmark(cp1 <- t(x) %*% x, cp2 <- crossprod(x), times=20)
boxplot(mb)
```

It is additionally simple to verify that these indeed compute the exact same thing:

```{r}
all.equal(cp1, cp2)
``` 



However, this implementation is not memory efficient, as it requires the allocation  of additional storage for:

1. `n`x`n` elements for the `crossprod()`
2. `n` elements for `diag()`
3. Another `n` elements just for the `sqrt()`
4. `n`x`n` elements for the `tcrossprod()`

The final output storage is the result of the division of `cp` by the result of `tcrossprod()`.   So the total number of *intermediary* elements allocated is `2n(n+1)`.  So for an input matrix with 1000 columns, you need `r matmemsize(1000)` of additional memory, and for one with 10000 columns, you need `r matmemsize(10000)` of additional storage.  For a smaller matrix, this may be satisfactory. 


### Comparison to fastco

On smaller datasets, the runtime efficient version can compete with the fastco version.  However, as the data size grows





# The Algorithms with Notes on Implementation

For dense implementations, the performance should scale well, and the non-BLAS components will use multiple threads (if your compiler supports OpenMP) when the matrix has more than 2500 columns. Additionally, we try to use vector operations (using OpenMP's new `simd` construct) for additional performance; but you need a compiler that supports a relatively modern OpenMP standard for this.


## Dense Matrix Input

Given an `m`x`n` matrix `x` (input) and an `n`x`n` matrix `cos` (preallocated output):

1. Compute the upper triangle of the crossproduct `cos = t(x) %*% x` using a symmetric rank-k update (the `_syrk` BLAS function).
2. Iterate over the upper triangle of `cos`:
    1. Divide its off-diagonal values by the square root of the product of its `i`'th and `j`'th diagonal entries.
    2. Replace its diagonal values with 1.
3. Copy the upper triangle of `cos` onto its lower triangle.

The total number of floating point operations is:

1. `m*n*(n+1)` for the symmetric rank-k update.
2. `3/2*(n+1)*n` for the rescaling operation.

The algorithmic complexity is `O(mn^2)`, and is dominated by the symmetric rank-k update. The storage complexity, ignoring the required allocation of outputs (such as the `cos` matrix), is `O(1)`.


## Dense Vector-Vector Input

Given two `n`-length vectors `x` and `y` (inputs):

1. Compute `crossprod = t(x) %*% y` (using the `_gemm` BLAS function).
2. Compute the square of the Euclidean norms of `x` and `y` (using the `_syrk` BLAS function).
3. Divide `crossprod` from 1 by the square root of the product of the norms from 2.

The total number of floating point operations is:

1. `2n-1` for the crossproduct.
2. `4*n-2` for the two (square) norms.
3. `3` for the division and square root/product.

The algorithmic complexity is `O(n)`. The storage complexity is `O(1)`.


## Sparse Matrix Input

Given an `m`x`n` sparse matrix stored as a COO with row/column indices `i` and `j` **where they are sorted by columns first, then rows**, and corresponding data `a` (inputs), and given a preallocated `n`x`n` dense matrix `cos` (output):

1. Initialize `cos` to 0.
2. For each column `j` of `a` (call it `x`), find its first and final position in the COO storage.
    1. If `x` is missing (its entries are all 0), set the `j`'th row and column of the lower triangle of `cos` to `NaN` (for compatibility with dense routines).  Go to 2.
    2. Otherwise, for each column `i>j` of `a` (call it `y`), find its first and final position  in the COO storage.
    3. Compute the dot product of `x` and `y`, and call it `xy`.
    4. If `xy > epsilon` (`epsilon=1e-10` for us):
        - Compute the dot products of `x` with itself `xx` and `y` with itself `yy`.
        - Set the `(i, j)`'th entry of `cos` to `xy / sqrt(xx*yy)`.
3. Copy the lower triangle to the upper and set the diagonal to 1.

The worst case run-time complexity occurs when the matrix is dense but stored as a sparse matrix, and is `O(mn^2)`, the same as in the dense case.  However, this will cause serious cache thrashing, and the performace will be abysmal.

The function stores the `j`'th column data and its row indices in temporary storage for better cache access patterns. Best case, this requires 12 KiB of additional storage, with 8 for the data and 4 for the indices.  Worse case (an all-dense column), this balloons up to `12m`. The storage complexity is best case `O(1)`, and worst case `O(m)`.



# Benchmarks

The source code for all benchmarks presented here can be found in the source tree of this package under `inst/benchmarks/`, or in the binary installation under `benchmarks/`.

All benchmarks were performed using:

* R 3.2.2
* OpenBLAS
* gcc 5.2.1
* 4 cores of a Core i5-2500K CPU @ 3.30GHz
* Linux kernel 4.2.0-16


## Dense Matrix Input

Compared to the version in the lsa package (as of 27-Oct-2015), this implementation performs quite well:

```r
library(rbenchmark)
reps <- 100
cols <- c("test", "replications", "elapsed", "relative")

m <- 2000
n <- 200
x <- matrix(rnorm(m*n), m, n)

benchmark(fastco::cosine(x), lsa::cosine(x), columns=cols, replications=reps)

##                   test replications elapsed relative
## 1 fastco::cosine(x)          100   0.177    1.000
## 2       lsa::cosine(x)          100 113.543  641.486
```


## Dense Vector-Vector Input

Here the two perform identically:

```r
library(rbenchmark)
reps <- 100
cols <- c("test", "replications", "elapsed", "relative")

n <- 1000000
x <- rnorm(n)
y <- rnorm(n)

benchmark(fastco::cosine(x, y), lsa::cosine(x, y), columns=cols, replications=reps)

##                      test replications elapsed relative
## 1 fastco::cosine(x, y)          100   0.757    1.000
## 2       lsa::cosine(x, y)          100   0.768    1.015
```


## Sparse Matrix Input

Benchmarking sparse matrix methods can be more challenging than with dense for a variety of reasons, chief among them being that the level of sparsity can make an enormous impact in performance.

We present two cases here of varying levels of sparsity.  First, we will examine the performance for a 0.1% dense / 99.9% sparse matrix:

```r
size <- .001*m*n

dense <- generate(m, n, size)
sparse <- as.simple_triplet_matrix(dense)

memuse(dense)
## 30.518 MiB
memuse(sparse)
## 63.508 KiB

benchmark(cosine(dense), cosine(sparse), as.matrix(sparse), columns=cols, replications=reps)
##                test replications elapsed relative
## 3 as.matrix(sparse)           30   1.416    1.000
## 1     cosine(dense)           30   4.146    2.928
## 2    cosine(sparse)           30   1.770    1.250
```

The performance is quite good for the sparse case, especially considering it uses one thread while the dense one uses 4. However, as the matris becomes more dense (and it doesn't take much), dense methods begin to perform better:


```r
size <- .01*m*n

dense <- generate(m, n, size)
sparse <- as.simple_triplet_matrix(dense)

memuse(dense)
## 30.518 MiB
memuse(sparse)
## 626.008 KiB

benchmark(cosine(dense), cosine(sparse), as.matrix(sparse), columns=cols, replications=reps)
##                test replications elapsed relative
## 3 as.matrix(sparse)           30   1.370    1.000
## 1     cosine(dense)           30   4.126    3.012
## 2    cosine(sparse)           30  12.348    9.013
```

So the total time here for the dense matrix (including the cast) is about 5.5 seconds, less than half of the 12.3 seconsd for the sparse case.  However, the memory usage for the dense case is greater by a factor of 50.

It is hard to give perfect advice for when to use a dense or sparse method, but a general rule of thumb is that if you have more than 5% non-zero data, don't even bother with sparse methods unless you absolutely must for storage purposes.





# References
<script language="JavaScript" src="include/headers.js"></script>
